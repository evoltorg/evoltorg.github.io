<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Using Apache To Stop Bad Robots</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">evolt.org</a></h1>
            <a class="extra" href="/">home</a>
          </div>

          <h2>Using Apache To Stop Bad Robots</h2>
<p class="meta">22 Aug 2001</p>

<div class="post">
<h3>The honest truth about bad robots</h3></p><p><p>\nFor just about as long as the commercial Internet has existed, SPAM email has been the bane of users worldwide. \nThe harder and harder we try \nto fight the spammers and keep our email addresses out of their hands, the smarter they get and the harder they fight back.\nOne example of peoples attempts to fight back is the large numbers of joe@NOSPAM.email.com, NO.mary.SPAM@REMOVESPAM.mary.com, etc email addresses\nyou find on Usenet and web based communities these days.  Worse yet, many people hold back from contributing\nto online discussions for fear their email address will be available for evil web spiders (I call them Spiderts - A web spider with a Catbert\ntype personality) to harvest and exploit from mailing list archives.\n</p>\n<p>\nAs one who runs(and uses!) evolt's mailing lists, keeping thousands of people's email addresses out of the tentacles of Spiderts has always \nbeen a big concern of mine. At first, it was easily remedied by using the %40 'trick'. Instead of writing archives with an easily \nrecognizable email address (abuse@aol.com for example), I had our mailing list software write all email addresses as \nabuse<strong>%40</strong>aol.com </p></p><p><p>This still allowed for a fairly easy to read address for humans while maintaining the ability to click the mailto: link\nand have one's associated email client create a new message with the correct email address entered. The Spiderts wouldn't recognize\nabuse<strong>%40</strong>aol.com as a valid email address and therefore not harvest it.\n</p>\n<p>\nThis was a fairly good solution until its use became widespread, at which point the creators of the Spiderts tweaked their \nunholy creations to recognize abuse%40aol.com as a harvestable email address and siphon it as well. As if it couldn't get worse, it was \nalso becoming apparent that the newer generations of Spiderts don't play by the rules set out for web spiders, and would disregard \nany "Disallow: /" entries in the robots.txt file. In fact, I've seen Spiderts that <em>only</em> go for what we specifically tell them \nnot to! What's a webmaster to do?!?\n</p></p><p><h3>Setting the trap</h3>\n<p>\nThe first step in our war against the Spiderts is to identify them. There are many techniques to find out who the bad bots are, from manually searching your access_logs to using a <a href="http://www.robotstxt.org/wc/active.html" target="_new">maintained list</a> and picking which ones you want to exclude.\nAt the end of the day it's getting the robots name - its <em>User-Agent</em> - that's important, not how you get it. That said, here's\na method I like that targets the worst offenders.</p></p><p><p>\nAdd a line like this to your robots.txt file:\n</p>\n<p>\n<code>Disallow: /email-addresses/</code>\n</p>\n<p>\nwhere 'email-addresses' is not a real directory. Wait a decent amount of time (a week to a month) then go through your access_log file and \npick out the User-Agent strings that accessed the /email-addresses/ directory. These are the worst of the worst - those that \nblatantly disregard our attempts to keep them out and fill our Inboxs with crap about lowering mortgage rates.  An easy way to get\na listing of those User-Agents that did access your fake directory (my examples are with grep and awk, win32 folks can check out\n<a href="http://www.redhat.com/products/tools/cygwin/" target="_new">Cygwin tools</a>) with a combined access_log format is \nwith the following command:</p> \n<p>\n<code>grep \/email-addresses access_log   awk '{print $12}'   uniq</code>\n</p>\n<p>\nThis simply searches the access_log file for any occurrences of /email-addresses, then prints the 12th column (<em>Where $12 is the column of your access_log that contains the User-Agent string</em>) of its results, then filters\nit down so only unique entries show. More on grep and awk can be found at the\n<a href="http://www.gnu.org/software/software.html#TOCDescriptionsOfGNUSoftware" target="_new">GNU software page</a>.\n</p>\n<p>\nNow that we have their identities, we can put the mechanisms in place to keep these hell-spawns away from our email addresses.\n</p></p><p><h3>Hook, line and sinker</h3>\n<p>\nHere are a couple of the User-Agents that fell for our trap that I pulled out of last months access_log for lists.evolt.org:\n</p>\n<p>\nWget/1.6<br>\nEmailSiphon<br>\nEmailWolf 1.00\n</p>\n<p>\nTo learn more about these and other web spiders, check out <a href="http://www.robotstxt.org" target="_new">http://www.robotstxt.org</a>.\n</p>\n<p>\nNow that we have the names of what these Spiderts go by, there are a couple ways to block them. You can use mod_rewrite as\ndescribed <a href="http://www.webtechniques.com/archives/2001/08/champeon/" target="_new">here</a>, but mod_rewrite can be difficult to configure\nand learn for many. It's also not compiled into Apache by default, which makes it slightly prohibitive.\n</p>\n<p>\nWe're going to use the <a href="http://httpd.apache.org/docs/env.html" target="_new">environment variable</a> features found in Apache to fight\nour battle, specifically the 'SetEnv' directive. This is a simple alternative to mod_rewrite and almost everything needed is compiled in to the webserver by default.\nIn this example, we're editing the httpd.conf file, but you should be able to use it in an .htaccess file as well.\n</p>\n<p>\nThe first line we add to our config file is:\n</p>\n<p>\n<code>SetEnvIfNoCase User-Agent "^Wget" bad_bot</code><br>\n<code>SetEnvIfNoCase User-Agent "^EmailSiphon" bad_bot</code><br>\n<code>SetEnvIfNoCase User-Agent "^EmailWolf" bad_bot</code><br>\n</p>\n<p>\nThe 'SetEnvIfNoCase' simply sets an enviornment <em>(SetEnv)</em> variable called 'bad_bot' If <em>(SetEnvIf)</em> the 'User-Agent' \nstring contains Wget, EmailSiphon, or EmailWolf, regardless of case <em>(SetEnvIfNoCase)</em>. In english, anytime a browser with a name containing 'wget, emailsiphon, or emailwolf' \naccesses our website, we set a variable called 'bad_bot'. We'd also want to add a line for the User-Agent string of any other Spidert we\nwant to deny.\n</p>\n<p>\nNow we tell Apache which directories to block the Spiderts from with the &lt;Directory&gt; directive:\n<br>\n<pre>\n&lt;Directory "/home/evolt/public_html/users/"&gt;\n        Order Allow,Deny\n        Allow from all\n        Deny from env=bad_bot\n&lt;/Directory&gt;\n</pre>\n</p>\n<p>\nIn english, we're denying access to the /home/lists/public_html/archive directory if the environment variable exists called 'bad_bot'. Apache\nwill return a standard <strong>403 Denied</strong> error message, and the Spidert gets nothing!\nSince most of the email addresses of members are found in lists.evolt.org/archive, this should suffice, but you'll\nprobably want to adjust a couple things to fit your needs.\n</p>\n<p>\nThere are many resources on the Web for discovering the User-Agent strings of Spiderts. The difficult part until now has been \nthe process of actually blocking them from your server. Thankfully, Apache provides us with the ability to easily block\nthose harbingers of SPAM from our servers and most importantly, our online identities.\n</p>
</div>


          <div class="footer">
            <div class="contact">
              <p>
                Your Name<br />
                What You Are<br />
                you@example.com
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="https://github.com/yourusername">github.com/yourusername</a><br />
                <a href="https://twitter.com/yourusername">twitter.com/yourusername</a><br />
              </p>
            </div>
          </div>
        </div>

    </body>
</html>
