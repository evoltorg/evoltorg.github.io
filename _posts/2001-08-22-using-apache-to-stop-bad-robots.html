---
layout: post
permalink: node/15126
ratings: 29
avgrate: 4.2414
category: Backend
user: djc
real_name: "Daniel Cody"
user_since: 14 Dec 1998
avatar: ""
article_count: 146
excerpt: "Tired of email harvesting robots roaming your site? Looking for a sure fire way to stop them? Using Apache, we'll discuss some easy and effective ways to do just that."
---
<h3>The honest truth about bad robots</h3></p><p><p></p>For just about as long as the commercial Internet has existed, SPAM email has been the bane of users worldwide. </p>The harder and harder we try </p>to fight the spammers and keep our email addresses out of their hands, the smarter they get and the harder they fight back.</p>One example of peoples attempts to fight back is the large numbers of joe@NOSPAM.email.com, NO.mary.SPAM@REMOVESPAM.mary.com, etc email addresses</p>you find on Usenet and web based communities these days.  Worse yet, many people hold back from contributing</p>to online discussions for fear their email address will be available for evil web spiders (I call them Spiderts - A web spider with a Catbert</p>type personality) to harvest and exploit from mailing list archives.</p></p></p><p></p>As one who runs(and uses!) evolt's mailing lists, keeping thousands of people's email addresses out of the tentacles of Spiderts has always </p>been a big concern of mine. At first, it was easily remedied by using the %40 'trick'. Instead of writing archives with an easily </p>recognizable email address (abuse@aol.com for example), I had our mailing list software write all email addresses as </p>abuse<strong>%40</strong>aol.com </p></p><p><p>This still allowed for a fairly easy to read address for humans while maintaining the ability to click the mailto: link</p>and have one's associated email client create a new message with the correct email address entered. The Spiderts wouldn't recognize</p>abuse<strong>%40</strong>aol.com as a valid email address and therefore not harvest it.</p></p></p><p></p>This was a fairly good solution until its use became widespread, at which point the creators of the Spiderts tweaked their </p>unholy creations to recognize abuse%40aol.com as a harvestable email address and siphon it as well. As if it couldn't get worse, it was </p>also becoming apparent that the newer generations of Spiderts don't play by the rules set out for web spiders, and would disregard </p>any "Disallow: /" entries in the robots.txt file. In fact, I've seen Spiderts that <em>only</em> go for what we specifically tell them </p>not to! What's a webmaster to do?!?</p></p></p><p><h3>Setting the trap</h3></p><p></p>The first step in our war against the Spiderts is to identify them. There are many techniques to find out who the bad bots are, from manually searching your access_logs to using a <a href="http://www.robotstxt.org/wc/active.html" target="_new">maintained list</a> and picking which ones you want to exclude.</p>At the end of the day it's getting the robots name - its <em>User-Agent</em> - that's important, not how you get it. That said, here's</p>a method I like that targets the worst offenders.</p></p><p><p></p>Add a line like this to your robots.txt file:</p></p></p><p></p><code>Disallow: /email-addresses/</code></p></p></p><p></p>where 'email-addresses' is not a real directory. Wait a decent amount of time (a week to a month) then go through your access_log file and </p>pick out the User-Agent strings that accessed the /email-addresses/ directory. These are the worst of the worst - those that </p>blatantly disregard our attempts to keep them out and fill our Inboxs with crap about lowering mortgage rates.  An easy way to get</p>a listing of those User-Agents that did access your fake directory (my examples are with grep and awk, win32 folks can check out</p><a href="http://www.redhat.com/products/tools/cygwin/" target="_new">Cygwin tools</a>) with a combined access_log format is </p>with the following command:</p> </p><p></p><code>grep \/email-addresses access_log   awk '{print $12}'   uniq</code></p></p></p><p></p>This simply searches the access_log file for any occurrences of /email-addresses, then prints the 12th column (<em>Where $12 is the column of your access_log that contains the User-Agent string</em>) of its results, then filters</p>it down so only unique entries show. More on grep and awk can be found at the</p><a href="http://www.gnu.org/software/software.html#TOCDescriptionsOfGNUSoftware" target="_new">GNU software page</a>.</p></p></p><p></p>Now that we have their identities, we can put the mechanisms in place to keep these hell-spawns away from our email addresses.</p></p></p><p><h3>Hook, line and sinker</h3></p><p></p>Here are a couple of the User-Agents that fell for our trap that I pulled out of last months access_log for lists.evolt.org:</p></p></p><p></p>Wget/1.6<br></p>EmailSiphon<br></p>EmailWolf 1.00</p></p></p><p></p>To learn more about these and other web spiders, check out <a href="http://www.robotstxt.org" target="_new">http://www.robotstxt.org</a>.</p></p></p><p></p>Now that we have the names of what these Spiderts go by, there are a couple ways to block them. You can use mod_rewrite as</p>described <a href="http://www.webtechniques.com/archives/2001/08/champeon/" target="_new">here</a>, but mod_rewrite can be difficult to configure</p>and learn for many. It's also not compiled into Apache by default, which makes it slightly prohibitive.</p></p></p><p></p>We're going to use the <a href="http://httpd.apache.org/docs/env.html" target="_new">environment variable</a> features found in Apache to fight</p>our battle, specifically the 'SetEnv' directive. This is a simple alternative to mod_rewrite and almost everything needed is compiled in to the webserver by default.</p>In this example, we're editing the httpd.conf file, but you should be able to use it in an .htaccess file as well.</p></p></p><p></p>The first line we add to our config file is:</p></p></p><p></p><code>SetEnvIfNoCase User-Agent "^Wget" bad_bot</code><br></p><code>SetEnvIfNoCase User-Agent "^EmailSiphon" bad_bot</code><br></p><code>SetEnvIfNoCase User-Agent "^EmailWolf" bad_bot</code><br></p></p></p><p></p>The 'SetEnvIfNoCase' simply sets an enviornment <em>(SetEnv)</em> variable called 'bad_bot' If <em>(SetEnvIf)</em> the 'User-Agent' </p>string contains Wget, EmailSiphon, or EmailWolf, regardless of case <em>(SetEnvIfNoCase)</em>. In english, anytime a browser with a name containing 'wget, emailsiphon, or emailwolf' </p>accesses our website, we set a variable called 'bad_bot'. We'd also want to add a line for the User-Agent string of any other Spidert we</p>want to deny.</p></p></p><p></p>Now we tell Apache which directories to block the Spiderts from with the &lt;Directory&gt; directive:</p><br></p><pre></p>&lt;Directory "/home/evolt/public_html/users/"&gt;</p>        Order Allow,Deny</p>        Allow from all</p>        Deny from env=bad_bot</p>&lt;/Directory&gt;</p></pre></p></p></p><p></p>In english, we're denying access to the /home/lists/public_html/archive directory if the environment variable exists called 'bad_bot'. Apache</p>will return a standard <strong>403 Denied</strong> error message, and the Spidert gets nothing!</p>Since most of the email addresses of members are found in lists.evolt.org/archive, this should suffice, but you'll</p>probably want to adjust a couple things to fit your needs.</p></p></p><p></p>There are many resources on the Web for discovering the User-Agent strings of Spiderts. The difficult part until now has been </p>the process of actually blocking them from your server. Thankfully, Apache provides us with the ability to easily block</p>those harbingers of SPAM from our servers and most importantly, our online identities.</p></p>