---
layout: post
permalink: The_Tao_of_Testing
ratings: 15
avgrate: 4.5333
user: MartinB
real_name: "Martin Burns"
user_since: 1999-04-26
avatar: "/images/pictures/picture-32.jpg"
article_count: 143
excerpt: "Fred Brooks says that a third of IT development time and effort should be spent in testing. In any software development project, with many people working on the coding, testing is essential to make sure that the system performs as the requirements say it should."
---
<p></p>Fred Brooks says</p>that a third of IT development time and effort </p>should be spent in testing. In any software development project,</p> with many people working on the coding, testing is essential to make</p>	sure that the system performs as the requirements say it should.</p></p></p><p></p>	However, even if you're a developer team of one, you still have an interest</p>	in ensuring that your work has proper Quality Assurance (QA) documentation</p>	for three main reasons:</p></p></p><ol></p>	<li></p>		Your future business depends entirely on your professional reputation -</p>		good clients will always look for a reputation for delivering their</p>		requirements. Anything which enhances that reputation is A Good Thing.</p>	</li></p>	<li></p>		Once the system is handed over to the client, you will then have an</p>		audit trail of testing, documenting that the system is working. If it</p>		later fails, you have a backup to safeguard you against potential</p>		legal and reputational action from a panicking client.</p>	</li></p>	<li></p>		If you want to feel self-interested about this (and most of us do at</p>		some point), remember that the client should pay for all of this testing -</p>		it's all chargeable time, which will result in the client getting a better</p>		system at the end of it.</p>	</li></p></ol></p><p></p>	So what do you have to do?</p></p></p><ul></p>	<li></p>		<a href="/The_Tao_of_Testing#scripts">Use Test Scripts</a></p>	</li></p>	<li></p>		<a href="/The_Tao_of_Testing#tests">Perform Tests</a></p>	<ul></p>		<li></p>			<a href="/The_Tao_of_Testing#usability">Usability Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#unit">Unit Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#system">System Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#integration">Integration Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#volume">Volume Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#regression">Regression Testing</a></p>		</li></p>		<li></p>			<a href="/The_Tao_of_Testing#uat">User Acceptance Testing (UAT)</a></p>		</li></p>	</ul></p>	</li></p>	<li></p>		<a href="/The_Tao_of_Testing#report">Report and Fix Errors</a></p>	</li></p></ul></p><h3><a name="scripts"></a>Test Scripts</h3></p><p></p>	Testing is a <strong>systematic</strong> discipline. You need to</p>	ensure that you test every piece of functionality against its specification, and that</p>	tests repeated after a bug has been fixed <em>are</em> the same</p>	as the test which highlighted the bug in the first place.</p></p></p><p></p>	The best way to ensure that there are no gaps in your test programme</p>	is to produce a test script. This will allow you to check that no</p>	area of functionality slips through the net, either at design stage,</p>	or while the tests are being performed. </p></p></p><p></p>	Your script should outline the steps which testers will follow,</p>	and list the expected results of each test. The detail you go into</p>	will depend on the time and budget available for testing.</p></p></p><p></p>	A sensible way of distributing the scripts is electronically - often</p>	as a word processing document. This will allow testers to record any</p>	errors which occur together with the tests which brought them out.</p>	You should archive the documents in read only format with the rest</p>	of the project documentation. To be on the safe side, the testers</p>	should print and sign the sheets, and again, you'll store these with</p>	the documentation.</p></p></p><h3><a name="tests"></a>Types of Testing</h3></p><dl></p>	<dt><a name="usability"></a><strong>Usability Testing</strong></dt></p>	<dd></p>		<p></p>			Usability testing should happen before a single element of </p>			the user interface (including information architecture) is</p>			fixed. Performing usability tests at this stage will allow</p>			you to change the interface reasonably quickly and cheaply -</p>			backing out an interface once it is coded is always going</p>			to be difficult.</p>		</p></p>		<p></p>			The best way to perform usability testing at this stage is</p>			to build a prototype of your proposed interface, and test</p>			that. Feedback from the testers will allow you to quickly</p>			amend your prototype and go through another iteration.</p>		</p></p>		<p></p>			Research shows that <a href="http://www.useit.com/alertbox/20000319.html" </p>			target="_foo" title="Opens in a new window">you only need to use five testers to</p>			perform the usability tests, and find 85% of the</p>			usability issues</a> in each iteration. After a few iterations,</p>			you're unlikely to have substantive issues left.</p>		</p></p>	</dd></p>	<dt><a name="unit"></a><strong>Unit Testing</strong></dt></p>	<dd></p>		<p></p>			Typically, a system contains a number of pieces such as</p>		</p></p>		<ul></p>			<li></p>				 'the bit which displays the product'</p>			</li>	 </p>			<li></p>				 'the bit which puts the product into the shopping cart'</p>			 </li></p>			<li></p>				 'the bit which which verifies the credit card and takes the payment'</p>			 </li></p>		 </ul></p>		 <p></p>			 and so on. Each of these is a unit, and you need to make sure</p>			 that each unit produces the appropriate output for the input you</p>			 give it, including sensible error trapping. A reasonably common</p>			 (but by no means the only) way of doing this might be at a</p>			 command-line, as this bypasses possible errors introduced by the</p>			 web server process itself. All you are doing is checking that the</p>			 basic code does what it says on the tin.</p>		</p></p>		<p></p>			Note that for complicated systems, each unit might be a system in its</p>			own right with sub-units. The division between system and unit tests in</p>			such a case is a little hazy.</p>		</p></p>	</dd></p>	<dt><a name="system"></a><strong>System Testing</strong></dt></p>	<dd></p>		<p></p>			Once you have all your units behaving as expected, you need to string</p>			them together into a system, and test it in a semi-real environment,</p>			which is only different from the way it will finally operate in that</p>			you're not working with real users and live data.</p>		</p></p>	</dd></p>	<dt><a name="integration"></a><strong>Integration Testing</strong></dt></p>	<dd></p>		<p></p>			As eBusinesses become more complicated, there is a growing need for</p>			the systems you produce to be integrated with other systems, like</p>			the financial reporting system, the logistics system, the customer</p>			database and so on.</p>		</p></p>		<p></p>			The purpose of integration testing is to ensure that your system's</p>			inputs from and outputs to the other systems are as expected. This</p>			means that you will need to ensure that test data fed between the</p>			systems is not going to be mistaken for live data. That said, at</p>			some point you will need to put a real transaction through your</p>			test system as an <strong>end-to-end test</strong>. A useful (and</p>			popular with developers) way of doing this is giving the team working</p>			on the site an allowance to spend on the site as 'friendly orders',</p>			in return for reporting back any customer-facing inconsistencies in</p>			the entire process.</p>		</p></p>	</dd></p>	<dt><a name="volume"></a><strong>Volume Testing</strong></dt></p>	<dd></p>		<p></p>			Far too often, an eBusiness is a victim of its own success. From</p>			<a href="http://whatis.techtarget.com/definition/0,289893,sid9_gci214064,00.html" </p>			target="_foo" title="Launches in a new window">the Slashdot effect</a> to </p>			<a href="http://www.cahoot.co.uk/" target="_foo" </p>			title="Cahoot offered 0% interest on the 1st 50,000 credit cards they issued. You couldn't access their site for a week after launch. Launches in a new window">sheer stupidity of the Marketing department</a>, if your system won't</p>			handle the loads put on it by users, you are going to lose both face</p>			and money. Larger eRetailers are now building their systems to handle</p>			over a thousand simultaneous users. While you may not be in that league,</p>			you need to simulate the loads you anticipate, plus leaving enough</p>			headroom for traffic growth. Get it wrong, and you may be facing </p>			<a href="http://www.if.com/" target="_foo" </p>			title="They launched their telesales in Summer 2000, but the web site didn't go live until December 2000, because their volume testing failed. Launches in a new window.">a </p>			launch delay of months</a>.</p>		</p></p>	</dd></p>	<dt><a name="regression"></a><strong>Regression Testing</strong></dt></p>	<dd></p>		<p></p>			Unless <span title="And even then I don't believe it">you are spectacularly </p>			lucky</span>, your testing will highlight errors in your system. And there's</p>			a better than average chance that fixing those errors will introduce new</p>			errors. Regression testing is a matter of going back over your previous</p>			tests to ensure that:</p>		</p></p>		<ol type="a"></p>			<li></p>				The bug you previously found <em>has</em> been fixed</p>			</li></p>			<li></p>				No new bugs have been introduced.</p>			</li></p>		</ol></p>		<p></p>			If you are producing release notes for each patch, it should be fairly easy</p>			to track down the cause of new errors introduced with a patch.</p>		</p></p>		<p></p>			The outcome of Regression is the inevitability that testing is an</p>			<strong>iterative</strong> discipline - you will need to continue to</p>			test, fix and regress until you have a system which meets the requirements.			</p>		</p></p>	</dd></p>	<dt><a name="uat"></a><strong>User Acceptance Testing (UAT)</strong></dt></p>	<dd></p>		<p></p>			Once you have what appears to you to be a working system, which meets</p>			all the requirements, the final piece of work you must undertake before</p>			you can ask for your cheque is User Acceptance Testing. This is essentially</p>			stepping through all the functionality with the client staff who are actually</p>			going to use the system.</p>		</p></p>		<p></p>			If your system fails UAT, yet meets the paper requirements, then you have</p>			an issue with your requirements documentation. You will need to resolve this</p>			with the client - has there been scope changes since the requirements doc</p>			was signed off? - before you can justifiably ask the client to sign off</p>			all your work and pay you.</p>		</p></p>	</dd></p></dl></p><p><h3><a name="report"></a>Report Errors and Fix</h3></p><p></p>	Once your testing has highlighted issues with the system,</p>	you need a process to ensure that each one is prioritised,</p>	diagnosed and fixed.</p></p></p><p></p>	A common approach is to have a central database which logs</p>	each new error, and captures the following information:</p></p></p><ul></p>	<li></p>		An ID number</p>	</li></p>	<li></p>		Status (new, in progress or resolved)</p>	</li></p>	<li></p>		Priority:</p>		<ol></p>			<li></p>				Red (ie causes non-functionality in the system.</p>				<strong>Must</strong> get fixed before go-live).</p>				I've also seen this subdivided into &quot;<cite>Red</cite>&quot; and </p>				&quot;<cite>Mother of Red</cite>&quot;.</p>			</li></p>			<li></p>				Amber (ie causes interference to user tasks. Should</p>				get fixed before go-live). </p>			</li></p>			<li></p>				Green (ie causes annoyance to users. Will get fixed if there is</p>				time before go-live).</p>			</li></p>		</ol></p>	</li></p>	<li></p>		Patch ID which will resolve (or has resolved) the error</p>	</li></p>	<li></p>		An owner - a named individual who will take responsibility for</p>		ensuring that the fix happens. This need not be the person who actually fixes it.</p>	</li></p>	<li></p>		A detailed description of the error, including any error messages,</p>		and screenshots where appropriate.</p>	</li></p></ul></p><p></p>	On each update of an error report, you should record an audit trail, outlining</p>	what's been changed, who's changed it and when.</p></p></p><p></p><strong>More info:</strong> <a href="http://www.amazon.co.uk/exec/obidos/asin/0201835959/martin043">The Mythical Man Month</a>, Fred Brooks.</p></p>