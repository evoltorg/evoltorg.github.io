---
layout: post
permalink: node/21392
ratings: 10
avgrate: 4.2000
user: djc
real_name: "Daniel Cody"
user_since: 14 Dec 1998
avatar: ""
article_count: 146
excerpt: "Last time, Dan showed you how to use the SetEnvIfNoCase directive within Apache to keep Spambots off your website. Read on for updated examples and methods that will keep  Spambots off your website and the SPAM out of your Inbox."
---
<p>In the initial article on <a href="/article/Using_Apache_to_stop_bad_robots/18/15126/index.html"</p>title="" target="_new">Using Apache to Stop Spambots</a>, we laid the foundation for using <a href="http://httpd.apache.org" title="Apache Home Page - Opens New Window" target="_blank">Apache</a> to identify, trap, and block Spambots (or <em>Spamberts</em>) from  your website. In this article, we'll be addressing some of the <a href="article/Using_Apache_to_stop_bad_robots/18/15126/index.html#comments" title="View the Comments from the First Article - Opens New Window" target="_blank">concerns brought up</a> by readers in response to the initial article. We'll also introduce some more full-proof tools, methods, and procedures for keeping the Spiderts out.</p></p><p><h2>A Recap</h2></p><p><p>Spiderts are evil. Since the original article in which we discussed some of the reasons to keep email harvesting robots off our sites was written nearly 5 months ago, the amount of SPAM people are receiving seems to have increased in frequency and intensified in its directness. Even the <a href="http://zdnet.com.com/2100-1106-827631.html" title="FTC to Regulate SPAM - Opens New Window" target="_blank">U.S. Government has finally started</a></p>to take notice and has <a href="http://zdnet.com.com/2100-1105-837448.html" title="Congress Moves to Protect ISPs - Opens New Window" target="_blank">started considering legislation</a> related to the subject. Other types of SPAM, such as the pornographic variety, are <a href="http://www.jsonline.com/bym/tech/news/feb02/20988.asp" title="Increase in adult-oriented junk messages angering millions - Opens New Window" target="_blank">increasingly thrust into the public spotlight</a> as it becomes more and more of a problem for the average Netizen.</p></p><p><p>In the first article, we showed how the <code>SetEnvIfNoCase</code> Apache directive could effectively be used to block Spiderts from your website by comparing their <strong>User-Agent</strong> string to a list that contained known Spidert <strong>User-Agents</strong>s or the <strong>User-Agent</strong>s that had accessed a fictitious directory within our robots.txt file. If the User-Agent was found within the banned list, an environment variable was set within Apache that would return a &quot;403 Denied&quot; error message to that User-Agent.</p></p><p><p>As many readers pointed out, however, what about those Spiderts that used common <strong>User-Agent</strong> strings such as &quot;Mozilla/4.0 (compatible; MSIE 5.01; Windows NT)&quot;?  After all, if some Spiderts are evil enough to index directories <strong>explicitly</strong> denied in the robots.txt file, surely they would have no problem using a fake <strong>User-Agent</strong> string to slip past our defences!</p></p><p><p>In this article, we'll discuss a more full-proof method to block the Spiderts from our websites by using their <strong>IP address</strong> - which is much harder to spoof - <em>instead</em> of their <strong>User-Agent</strong> which can be modified to match commonly used strings.</p></p><p><h2>The Setup</h2></p><p><p>As we did last time, we'll be using a &quot;honey-pot&quot; directory within our <a href="/evolt/robots.txt" title="See evolt.org's robots.txt file - Opens New Window" target="_blank">robots.txt</a> file to lure the Spiderts into a non-existent directory. Here's an example honey-pot robots.txt file:</p></p><p><pre></p>User-agent: *</p>Disallow: /user/</p># Here's our honey-pot directory, which doesn't actually exist.</p>Disallow: /email-addresses/</p></pre></p><p><p>What we'll be doing <em>differently</em> this time is blocking their <strong>IP address</strong>, and <em>not</em> their <strong>User-Agent</strong> string. As an added little bonus, we're also going to be using a Perl script to automatically parse our access_log and update a list of banned IP addresses which Apache</p>will be reading! This will alleviate the problem of catching a Spidert &quot;after the fact&quot; by <em>dynamically</em> sending the "403 Denied" error message</p>the first time it tries to access the honey-pot directory. All right! Let's move on...</p></p><p><h2>The Flypaper</h2></p><p><p>Here is the Perl script that will traverse our access_log looking for IP addresses which access the honey-pot directory <small>(Thanks to <a href="/user/dmah/92/index.html" title="Check out Dean's profile - Opens New Window" target="_blank">Dean Mah</a> for coding the Perl script for me!)</small>:</p></p><p><pre></p>#!/usr/bin/perl -w</p>#</p># greplog.pl - monitor the access_log for agents accessing the honey-pot</p>#              and block their IP addresses.</p>#</p># Notes:</p>#</p># - The log format is currently hardcoded.  Could grab this from the Apache</p>#   configuration file.</p>#</p>#</p># DSM - Dean Mah (dmah at members.evolt.org)</p><p>use strict;</p><p>use File::Tail;</p><p>$  = 1;</p><p># Configuration.</p><p>my $access_log = '/usr/local/apache/logs/access_log';</p>my $pidfile = '/usr/local/apache/logs/httpd.pid';</p>my $block_file = '/usr/local/apache/conf/badbots.txt';</p><p># Constants.</p><p>my $SIGUSR1 = 10;</p><p># Open the access log.</p>my $file = File::Tail->new($access_log);</p><p>my $pid = '';</p><p># Grab a list of already blocked IP addresses.</p>my %blocked = %{ init_blocked($block_file) };</p><p>while (defined($_ = $file->read)) {</p>    my ($ip, $path, $agent) = m ^([\d\.]+).+GET\s+(\S+).+\&quot;([^\&quot;]+)\&quot;$ ;</p><p>    if ($path =~ /email-addresses/) {</p><p>        # If the IP hasn't been blocked already, add it to the blocked</p>        # IP list and restart the server.</p><p>        if (!defined($blocked{$ip})) {</p>            if (open(FILE, &quot;&gt;&gt; $block_file&quot;)) {</p>                print FILE &quot;SetEnvIfNoCase Remote_Addr $ip bad_bot</p>&quot;;</p>                close(FILE);</p>            }</p><p>            $blocked{$ip} = 1;</p><p>            # Get Apache to re-read its configuration.  We need to</p>            # keep grabbing Apache's pid in case it changes while running.</p>            kill $SIGUSR1, $pid if (($pid = get_httpd_pid($pidfile)) != 0);</p>        }</p>    }</p>}</p><p>exit(0);</p>########## Subroutines ##########</p><p>#</p># init_blocked</p>#</p><p>sub init_blocked {</p>    my $block_file = shift;</p><p>    my %blocked = ();</p><p>    if (open(FILE, $block_file)) {</p>        while (&lt;FILE&gt;) {</p>            chomp;</p>            $blocked{$_} = 1;</p>        }</p>        close(FILE);</p>    }</p><p>    return \%blocked;</p>}</p><p>#</p># get_httpd_pid</p>#</p>sub get_httpd_pid {</p>    my $pidfile = shift;</p><p>    open(FILE, $pidfile)    return 0;</p>    chomp(my $pid = &lt;FILE&gt;);</p>    close(FILE);</p><p>    return $pid;</p>}</p></pre></p><p><p>First, we define the PATH to the access_log, the PID file for Apache, and the name of the file we're going to write the IP addresses of the offending Spiderts to.  Adjust these variables to taste, as they'll more than likely vary from system to system.</p></p><p><p>While the dissection of the Perl script is outside the scope of this article, let's run through it quick so you can get an idea of exactly what we're doing.</p></p><p><p>Basically it looks at the access_log and if the &quot;email-addresses&quot; (Remember, this is our honey-pot directory from the robots.txt file.) string is anywhere in the GET string of the access_log, it will add a new environment variable to the badbots.txt file using the <code>SetEnvIfNoCase</code> directive that contains the IP address of the client who's poking its nose where it shouldn't be. If it's a new IP address, the Perl script will restart Apache so it can re-read the badbots.txt file and block out the new entry. If you'd like to get creative, adding a sub-routine that emails an administrator each time a new IP address is blocked might be a nice idea.</p></p><p><p>As is, the Perl script will  continously 'tail' the access_log file, but you may want to modify it to run as a <a href="http://evolt.org/article/Cron_a_regular_workhorse/18/125/index.html" title="Learn to use cron" target="_new">cron job</a>, or manually run it yourself from time to time. Whatever floats your boat!</p></p><p><h2>Wrapping Up</h2></p><p><p>In our Apache httpd.conf file, we're going to include the file containing the blocked IP addresses with the <code>Include</code> directive like so:</p></p><p><pre></p># Include $APACHE_HOME/conf/badbots.txt</p>Include conf/badbots.txt</p></pre></p><p><p>And just in case you forgot from the last article, we would use the following code to block any clients that match the &quot;bad_bot&quot; environment variable from a directory:</p></p><p><pre></p>&lt;Directory &quot;/home/djc/public_html/*&quot;&gt;</p>        Order Allow,Deny</p>        Allow from all</p>        Deny from env=bad_bot</p>&lt;/Directory&gt;</p></pre></p><p><p>It should also be mentioned that the Perl script could easily be modified to output the IP address of the Spidert to a firewall rule that would <strong>completely</strong> block any TCP/IP traffic from the offending IP address to the server. That exercise is left to the reader as it doesn't have too much to do with Apache!</p></p><p><p>When you've got everything configured, just restart Apache, run the Perl script, and feel confident that just because your email address is on a publicly-available website, it's <em>much less</em> likely to be harvested and abused. And while there is no full-proof method to keep your email address completely out of the hands of SPAMers, we can (and will!) keep refining our techniques to make it as difficult as possible.</p></p><p><p>Have suggestions or comments? I'd love to hear what you have to say below!</p>